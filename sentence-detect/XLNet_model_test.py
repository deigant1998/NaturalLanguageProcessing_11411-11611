# -*- coding: utf-8 -*-
"""test2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KDIXhdGqJz6wsNAy9hEq-2bGjp6gGgUu
"""

!pip install transformers

!pip install SentencePiece

from transformers import XLNetTokenizer, XLNetModel
import torch.nn as nn
from torch.optim import Adam
import pandas as pd
import torch
import os
import random
import math

os.environ['CUDA_VISIBLE_DEVICES'] ='0'

def optimizer_to(optim, device):
    for param in optim.state.values():
        # Not sure there are any global tensors in the state dict
        if isinstance(param, torch.Tensor):
            param.data = param.data.to(device)
            if param._grad is not None:
                param._grad.data = param._grad.data.to(device)
        elif isinstance(param, dict):
            for subparam in param.values():
                if isinstance(subparam, torch.Tensor):
                    subparam.data = subparam.data.to(device)
                    if subparam._grad is not None:
                        subparam._grad.data = subparam._grad.data.to(device)

'''
class SentenceDetectionModel(nn.Module):
    def __init__(self, num_sentences):
        super(SentenceDetectionModel, self).__init__()
        self.tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased", do_lower_case=True)
        self.xlnetModel = XLNetModel.from_pretrained('xlnet-base-cased').to(torch.device("cuda:0"), dtype=torch.half, non_blocking=True)
        self.output = nn.Linear(10 * 768, num_sentences)
        self.softmax = nn.Softmax(-1)

        #Freeze the parameters of the model.
        for param in self.xlnetModel.base_model.parameters():
            param.requires_grad = False
             
    def forward(self, input):
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        input = input.to(device)
        model_outputs = self.xlnetModel(**input)
        #print(model_outputs.last_hidden_state.shape)
        #print(model_outputs.last_hidden_state.reshape(1, self.xlnetModel.config.hidden_size * 547))
        
        sentence_section_output = model_outputs.last_hidden_state[:, 0:480, :]
        
        #print(sentence_section_output)
        #print(torch.mean(sentence_section_output[:,0:48, :1], 1))
         
        sentence_embeddings = torch.zeros((1, 10, 768))
        
        for i in range(10):
            sentence_start = (i*48)
            sentence_end = ((i+1)*48)
            #print(sentence_section_output[:,  sentence_start:sentence_end  , :])
            #exit()
            sentence_tensor = torch.mean(sentence_section_output[:,  sentence_start:sentence_end  , :], 1)
            sentence_embeddings[0][i] = sentence_tensor
            
        #print(sentence_embeddings)
        #print(sentence_embeddings.shape)
        
        #print(sentence_embeddings[0, :])
        #print(sentence_section_output[0, 0:10, 0])
        
        #print(sentence_embeddings[0, :].shape)
        #print(torch.sum(sentence_section_output[0, 0:10, 0]))
        
        #print(sentence_embeddings)
        #print(sentence_embeddings.reshape(1, 10 * 768))
        
        output = self.output(sentence_embeddings.reshape(1, 10 * 768))
        classification = self.softmax(output)
        return classification
        
    def train(self, epochs, train_data):
        
        #some code borrowed from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html
        
        loss_function = nn.BCELoss()
        optimiser = Adam(self.parameters(), lr=2e-5)
        optimizer_to(optimiser, "cuda:0")
        
        for i in range(epochs):
            running_loss = 0
            
            for index, (data, label) in enumerate(train_data):
                #data = data.to(device)
                #label = label.to(device)
                optimiser.zero_grad()
                
                out = self(data)
                #print(data)
                #print(out)
                #print(label)
                topk, tok_index = torch.topk(out[0], int(torch.sum(label[0])))
                topk_label, tok_index_label = torch.topk(label[0], int(torch.sum(label[0])))
                #print(tok_index)
                #print(tok_index_label)
                #print(self.output.parameters)
                loss = loss_function(out, label)
                #print(loss.item())
                loss.backward()
                optimiser.step()
                #print(self.output.parameters)
                
                running_loss += loss.item()
                if index % 10 == 9:    # print every 10 mini-batches
                    print(f'[{index + 1}, {index + 1:5d}] loss: {running_loss / 10:.3f}')
                    running_loss = 0.0
    
                
    def extract_data_from_file(self, file_path):
        data = pd.read_excel(file_path)
        train_data = []
        for row in data.iloc:
            labels = row["Valid Sentences"]
            label_tensor = torch.zeros(10)
            for label in str(labels).split(","):
                parsed_label = int(label.strip())
                label_tensor[parsed_label] = 1
            label_tensor = label_tensor.reshape(1, 10)  
            question = row["question"]
            
            sentences = []
            for sent in str(row["Passage (10 sentences)"]).split('\n'):
                sentences.append(sent)
              
            if(len(sentences) != 10):
                raise("Invalid number of sentences")
            
            tokenized_question = self.tokenizer(question, max_length = 32, padding = 'max_length', truncation = True, add_special_tokens=False, return_tensors='pt')
            tokenized_sentences = self.tokenizer(sentences, max_length = 48, padding = 'max_length', truncation = True, add_special_tokens=False, return_tensors='pt')
            tokenized_sentences['input_ids'] = tokenized_sentences.input_ids.reshape(1, tokenized_sentences.input_ids.shape[0] * tokenized_sentences.input_ids.shape[1])
            tokenized_sentences['token_type_ids'] = tokenized_sentences.token_type_ids.reshape(1, tokenized_sentences.token_type_ids.shape[0] * tokenized_sentences.token_type_ids.shape[1])
            tokenized_sentences['attention_mask'] = tokenized_sentences.attention_mask.reshape(1, tokenized_sentences.attention_mask.shape[0] * tokenized_sentences.attention_mask.shape[1])
            tokenized_sentences['input_ids'] = torch.cat((tokenized_sentences['input_ids'], self.tokenizer("", return_tensors='pt').input_ids[:,0].reshape(1,1)), dim = 1)
            tokenized_sentences['token_type_ids'] = torch.cat((tokenized_sentences['token_type_ids'], torch.tensor([[0]])), dim = 1)
            tokenized_sentences['attention_mask'] = torch.cat((tokenized_sentences['attention_mask'], torch.tensor([[1]])), dim = 1)
            tokenized_sentences['input_ids'] = torch.cat((tokenized_sentences['input_ids'], tokenized_question.input_ids), dim=1)
            tokenized_sentences['attention_mask'] = torch.cat((tokenized_sentences['attention_mask'], tokenized_question.attention_mask), dim=1)
            tokenized_question.token_type_ids[tokenized_question.token_type_ids == 0] = 1
            tokenized_sentences['token_type_ids'] = torch.cat((tokenized_sentences['token_type_ids'], tokenized_question.token_type_ids), dim=1)
            tokenized_sentences['input_ids'] = torch.cat((tokenized_sentences['input_ids'], self.tokenizer("", return_tensors='pt').input_ids), dim = 1)
            tokenized_sentences['token_type_ids'] = torch.cat((tokenized_sentences['token_type_ids'], torch.tensor([[1,2]])), dim = 1)
            tokenized_sentences['attention_mask'] = torch.cat((tokenized_sentences['attention_mask'], self.tokenizer("", return_tensors='pt').attention_mask), dim = 1)
            #print(self.tokenizer.decode(tokenized_sentences.input_ids[0][0:400]))
            #exit()
            datapoint = (tokenized_sentences, label_tensor)
            train_data.append(datapoint)
        return train_data
'''

class SentenceDetectionModel(nn.Module):
    def __init__(self, num_sentences):
        super(SentenceDetectionModel, self).__init__()
        self.tokenizer = XLNetTokenizer.from_pretrained("xlnet-large-cased", do_lower_case=True)
        self.xlnetModel = XLNetModel.from_pretrained('xlnet-large-cased').to(torch.device("cuda:0"), dtype=torch.half, non_blocking=True)
        self.hidden = nn.Linear(11 * 1024, 500)
        self.activation = nn.Tanh();
        self.out = nn.Linear(500, num_sentences)
        self.softmax = nn.Softmax(-1)

        #Freeze the parameters of the model.
        for param in self.xlnetModel.base_model.parameters():
            param.requires_grad = False
             
    def forward(self, input):
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        input = input.to(device)
        model_outputs = self.xlnetModel(**input)

        sentence_section_output = model_outputs.last_hidden_state[:, 0:480, :]
        sentence_embeddings = torch.zeros((1, 10, 1024))
        
        for i in range(10):
            sentence_start = (i*48)
            sentence_end = ((i+1)*48)
            sentence_tensor = torch.max(sentence_section_output[:,  sentence_start:sentence_end  , :], 1).values
            sentence_embeddings[0][i] = sentence_tensor
        
        question_tensor = torch.max(model_outputs.last_hidden_state[:,481:513,:], 1).values
        question_tensor = question_tensor.to('cpu')
        sentence_embeddings = sentence_embeddings.to('cpu')
        concat_sentence_question = torch.cat((sentence_embeddings, 
                                              question_tensor.reshape(1, question_tensor.shape[0], 
                                                                      question_tensor.shape[1])), dim=1)
        
        hidden = self.hidden(concat_sentence_question.reshape(1, 11 * 1024))
        activated = self.activation(hidden)
        output = self.out(activated)
        classification = self.softmax(output)
        return classification
        
    def train(self, epochs, train_data):
        
        #some code borrowed from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html
        
        loss_function = nn.BCELoss(reduction='sum')
        optimiser = Adam(self.parameters(), lr=2e-5)
        optimizer_to(optimiser, "cuda:0")
        
        for i in range(epochs):
            running_loss = 0
            print("starting epoch " + str(i))
            
            for index, ((question, sentences), (data, label)) in enumerate(train_data):
                optimiser.zero_grad()
                
                out = self(data)
                #print(out)
                #print((out[0] > 0.1).nonzero().reshape(1, (out[0] > 0.1).nonzero().shape[0]))
                #print(label)
                #topk, tok_index = torch.topk(out[0], int(torch.sum(label[0])))
                #topk_label, tok_index_label = torch.topk(label[0], int(torch.sum(label[0])))
                #print(tok_index)
                #print(tok_index_label)
                #print(self.output.parameters)
                loss = loss_function(out, label)
                #print(loss)
                loss.backward()
                optimiser.step()
                #print(self.output.parameters)
                
                running_loss += loss.item()
                if index % 10 == 9:    # print every 10 mini-batches
                    print(f'[{index + 1}, {index + 1:5d}] loss: {running_loss / 10:.3f}')
                    running_loss = 0.0
    
                
    def extract_data_from_file(self, file_path):
        data = pd.read_excel(file_path)
        train_data = []
        for row in data.iloc:
            labels = row["Valid Sentences"]
            label_tensor = torch.zeros(10)
            for label in str(labels).split(","):
                parsed_label = int(label.strip())
                label_tensor[parsed_label] = 1
            label_tensor = label_tensor.reshape(1, 10)  
            question = row["question"]
            #print(str(row["Passage (10 sentences)"]).replace(r'\\n', '\n'))
            #passage = self.nlp(str(row["Passage (10 sentences)"]).replace(r'\\n', '\n'))
            sentences = []
            for sent in str(row["Passage (10 sentences)"]).split('\n'):
                sentences.append(sent)
                
            #print(sentences)
            #print(len(sentences))
            if(len(sentences) != 10):
                print(sentences)
                raise("Invalid number of sentences")
            
            tokenized_question = self.tokenizer(question, max_length = 32, padding = 'max_length', truncation = True, add_special_tokens=False, return_tensors='pt')
            tokenized_sentences = self.tokenizer(sentences, max_length = 48, padding = 'max_length', truncation = True, add_special_tokens=False, return_tensors='pt')
            tokenized_sentences['input_ids'] = tokenized_sentences.input_ids.reshape(1, tokenized_sentences.input_ids.shape[0] * tokenized_sentences.input_ids.shape[1])
            tokenized_sentences['token_type_ids'] = tokenized_sentences.token_type_ids.reshape(1, tokenized_sentences.token_type_ids.shape[0] * tokenized_sentences.token_type_ids.shape[1])
            tokenized_sentences['attention_mask'] = tokenized_sentences.attention_mask.reshape(1, tokenized_sentences.attention_mask.shape[0] * tokenized_sentences.attention_mask.shape[1])
            tokenized_sentences['input_ids'] = torch.cat((tokenized_sentences['input_ids'], self.tokenizer("", return_tensors='pt').input_ids[:,0].reshape(1,1)), dim = 1)
            tokenized_sentences['token_type_ids'] = torch.cat((tokenized_sentences['token_type_ids'], torch.tensor([[0]])), dim = 1)
            tokenized_sentences['attention_mask'] = torch.cat((tokenized_sentences['attention_mask'], torch.tensor([[1]])), dim = 1)
            tokenized_sentences['input_ids'] = torch.cat((tokenized_sentences['input_ids'], tokenized_question.input_ids), dim=1)
            tokenized_sentences['attention_mask'] = torch.cat((tokenized_sentences['attention_mask'], tokenized_question.attention_mask), dim=1)
            tokenized_question.token_type_ids[tokenized_question.token_type_ids == 0] = 1
            tokenized_sentences['token_type_ids'] = torch.cat((tokenized_sentences['token_type_ids'], tokenized_question.token_type_ids), dim=1)
            tokenized_sentences['input_ids'] = torch.cat((tokenized_sentences['input_ids'], self.tokenizer("", return_tensors='pt').input_ids), dim = 1)
            tokenized_sentences['token_type_ids'] = torch.cat((tokenized_sentences['token_type_ids'], torch.tensor([[1,2]])), dim = 1)
            tokenized_sentences['attention_mask'] = torch.cat((tokenized_sentences['attention_mask'], self.tokenizer("", return_tensors='pt').attention_mask), dim = 1)
            datapoint = ((question, sentences), (tokenized_sentences, label_tensor))
            train_data.append(datapoint)
        return train_data

sentenceDetection = SentenceDetectionModel(10)
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(device)
#sentenceDetection = sentenceDetection.to(device)
#print(sentenceDetection.device)
data = sentenceDetection.extract_data_from_file("/content/Book3.xlsx")
#data = data.to(device)

random.shuffle(data)
train_size = math.floor(0.8*len(data))
train_dataset, test_dataset = data[:train_size], data[train_size:]

sentenceDetection.train(60, train_dataset)

for index, ((question, sentences), (data, label)) in enumerate(train_dataset[0:10]):
    train_out = sentenceDetection(data)
    print(question, sentences)
    print(label)
    print(train_out)
    print((train_out[0] > 0.1).nonzero().reshape(1, (train_out[0] > 0.1).nonzero().shape[0]))

for index, ((question, sentences), (data, label)) in enumerate(test_dataset):
    test_out = sentenceDetection(data)
    print(question, sentences)
    print(label)
    print(test_out)
    print((test_out[0] > 0.1).nonzero().reshape(1, (test_out[0] > 0.1).nonzero().shape[0]))